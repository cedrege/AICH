{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook to create submission and calculate local score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Jupyter notebook performs the state predictions and extracts the events based on the preprocessed data and the trained model. Afterwards it calcualts the mean average precision score and exports the events as submission.csv."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Before running this notebook, please ensure that the directory `../data/engineered/train` contains the trainig data and that `../data/engineered/val` contains the validation data from the feature engineering notebook. Also make sure that the model and the scaler are trained from the model notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install matplotlib joblib pandas numpy pyarrow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import load\n",
    "import pandas as pd\n",
    "import pyarrow.dataset as ds\n",
    "from score import *\n",
    "from event_extraction_function import *\n",
    "from plot_function import *\n",
    "from columns_drop import *\n",
    "from warnings import filterwarnings\n",
    "\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(name: str):\n",
    "    with open(name, 'rb') as f:\n",
    "        return load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model and scaler\n",
    "M1_PATH = 'models/model_randomforestclassifier-both-n_estimators__500-max_depth__40-min_samples_leaf__15-random_state__42-n_jobs__10-warm_start__true.jlb'\n",
    "S_PATH  = 'models/scaler_randomforestclassifier-wrist-n_estimators__500-max_depth__20-min_samples_leaf__15-random_state__42-n_jobs__10-warm_start__true.jlb'\n",
    "\n",
    "m1 = load_model(M1_PATH)\n",
    "s  = load_model(S_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "NEW_TRAIN_DATA = '../data/engineered/train'\n",
    "NEW_VALIDATION_DATA = '../data/engineered/val'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting states und extracting events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batched_dataloader(validation=True, batch_size=100_000):\n",
    "    dataset = ds.dataset(NEW_VALIDATION_DATA if validation else NEW_TRAIN_DATA)\n",
    "    batch = pd.DataFrame()\n",
    "    for file_batch in dataset.to_batches(batch_size=batch_size):\n",
    "        batch = pd.concat([batch, file_batch.to_pandas()])\n",
    "        if len(batch) >= batch_size:\n",
    "            yield batch.reset_index(drop=True)\n",
    "            batch = pd.DataFrame()\n",
    "    yield batch.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 10_000_000\n",
    "\n",
    "submission_df = pd.DataFrame(columns=['series_id', \n",
    "                                    'step', \n",
    "                                    'event', \n",
    "                                    'score', \n",
    "                                    'probability', \n",
    "                                    'timestamp',\n",
    "                                    'enmo',\n",
    "                                    'remove_events'])\n",
    "\n",
    "\n",
    "\n",
    "# True for validation, False for training\n",
    "for batch in batched_dataloader(True, BATCH_SIZE):\n",
    "\n",
    "    batch.reset_index(inplace=True, drop=True)\n",
    "    series_id_minutes = batch[['series_id','minute']]\n",
    "    batch = batch.drop(columns_to_drop, axis=1)\n",
    "    batch = pd.DataFrame(s.transform(batch), columns=batch.columns)\n",
    "    \n",
    "    # predict probability for awake\n",
    "    pred = m1.predict_proba(batch)\n",
    "    batch = pd.DataFrame(s.inverse_transform(batch), columns=batch.columns)\n",
    "    batch['probability'] = pred[:, 1] \n",
    "    \n",
    "    # define where to make the cut\n",
    "    batch['pred_awake'] = batch['probability'].apply(lambda x: 1 if x >= 0.65 else 0)\n",
    "    \n",
    "    # prepare df for event extraction function\n",
    "    batch['series_id'] = series_id_minutes['series_id']\n",
    "    batch['minute'] = series_id_minutes['minute']\n",
    "    \n",
    "    # apply heuristic: mean_boolean decides if score should be calculated by the mean of the probabilites\n",
    "    pre_sub = heuristic_function(batch, period_1 = 30, period_2 = 30)\n",
    "    \n",
    "    # append to submission_df\n",
    "    submission_df = pd.concat([submission_df, pre_sub[['series_id', 'step', 'event', 'score', 'probability', 'timestamp', 'enmo', 'remove_events']]])\n",
    "    \n",
    "    # remove events in nights with repetitve movement patterns\n",
    "    submission_df = submission_df[submission_df['remove_events'] != True]\n",
    "\n",
    "\n",
    "submission_df = submission_df.reset_index(drop=True).reset_index(names=\"row_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solution = pd.read_csv('../data/train_events.csv')\n",
    "solution.dropna(subset=['step'], inplace=True)\n",
    "sample_val = solution[solution['series_id'].isin(submission_df.series_id.unique())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sleep_tolerances = {\n",
    "    'onset': [12, 36, 60, 90, 120, 150, 180, 240, 300, 360],  \n",
    "    'wakeup': [12, 36, 60, 90, 120, 150, 180, 240, 300, 360]  \n",
    "}\n",
    "score = score(\n",
    "        solution = sample_val,\n",
    "        submission = submission_df,\n",
    "        tolerances = sleep_tolerances,\n",
    "        series_id_column_name = 'series_id',\n",
    "        time_column_name = 'step',\n",
    "        event_column_name = 'event',\n",
    "        score_column_name = 'score',\n",
    "        use_scoring_intervals =False,\n",
    ")\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the submission vs. the solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for a visualization of all series remove [:100]\n",
    "train_series = pd.read_parquet(\"../data/train_series.parquet\")\n",
    "for series_id in sample_val['series_id'][:100].unique():\n",
    "    plot_whole_series(series_id, train_series, sample_val, submission_df, 'lightgrey', 'blue', 'green', font_size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export the submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df.drop(['probability', 'timestamp', 'enmo', 'remove_events'], axis = 1, inplace=True)\n",
    "submission_df.to_csv('submission.csv', index=False, encoding='utf-8', lineterminator='\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-aich",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
