{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to solve the kaggle competition \"Child Mind Institute - Detect Sleep States\"\n",
    "Link to the competition: https://www.kaggle.com/competitions/child-mind-institute-detect-sleep-states\n",
    "\n",
    "\n",
    "> Before running this notebook, please ensure that the directory `../data/engineered/train` contains the trainig data and that `../data/engineered/val` contains the validation data from the feature engineering notebook!\n",
    "\n",
    "\n",
    "If you only want to train the final model we used during the competition, please read the second to last cell in this Notebook!\n",
    "\n",
    "\n",
    "### Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install matplotlib plotly pandas numpy tqdm scikit-learn pyarrow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "from tqdm import tqdm\n",
    "from score import score\n",
    "#from sklearn.svm import SVC\n",
    "#from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import recall_score, precision_score, average_precision_score, f1_score\n",
    "import wandb\n",
    "from abc import ABC, abstractmethod #, classmethod\n",
    "from math import ceil, floor\n",
    "import os\n",
    "from pathlib import Path\n",
    "from joblib import dump, load\n",
    "from custom_enums import ModelTrainingType\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow.dataset as ds\n",
    "import gc\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# wandb.login() requires you to get your API key from your account settings\n",
    "# open the weights and biases website https://wandb.ai/login and login to your account\n",
    "# then go to your account settings and copy the API key\n",
    "# paste it in the input box and hit enter\n",
    "\n",
    "#wandb.login() #TODO: uncomment this line to login to your account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # start a new wandb run to track this script\n",
    "# wandb.init(\n",
    "#     # set the wandb project where this run will be logged\n",
    "#     project=\"my-awesome-project\",\n",
    "    \n",
    "#     # track hyperparameters and run metadata\n",
    "#     config={\n",
    "#     \"learning_rate\": 0.02,\n",
    "#     \"architecture\": \"CNN\",\n",
    "#     \"dataset\": \"CIFAR-100\",\n",
    "#     \"epochs\": 10,\n",
    "#     }\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to switch between different models from different libraries at a glance, we implement an interface called `IPipelineRequirements`. This allows us to make the pipleine even more robust and easier to extend upon. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IPipelineRequirements(ABC):\n",
    "    @abstractmethod\n",
    "    def train(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def save(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def load(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def evaluate(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first model is the baseline model, which just takes the mean over all onset and wakeup times and tries to predict `onset` and `wakeup` events with the calculated time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineModel(IPipelineRequirements):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def train(self):\n",
    "        raise NotImplementedError(\"Please Implement this method\")\n",
    "\n",
    "    def predict(self):\n",
    "        raise NotImplementedError(\"Please Implement this method\")\n",
    "\n",
    "    def save(self):\n",
    "        raise NotImplementedError(\"Please Implement this method\")\n",
    "\n",
    "    def load(self):\n",
    "        raise NotImplementedError(\"Please Implement this method\")\n",
    "\n",
    "    def evaluate(self):\n",
    "        raise NotImplementedError(\"Please Implement this method\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = BaselineModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numerous studies have concentrated on applications using `RandomForest`. The primary motivation for this preference is the model's inherent transparency in decision-making processes, which are readily identifiable in such models. Subsequent to the BaselineModel, efforts have been made to abstract models from the Scikit-learn library. Fortunately, the majority of models within their API exhibit consistent implementation patterns, facilitating their integration into the processing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkLearnModel(IPipelineRequirements):\n",
    "\n",
    "    def __init__(self, model_path):\n",
    "        self.load(model_path)\n",
    "\n",
    "    def __init__(self, model, identifier, scaler=StandardScaler, sk_model_params=None):\n",
    "        self._model = model\n",
    "        self._scaler = scaler if not callable(scaler) else scaler() # if scaler is a classpointer, instantiate it\n",
    "        self.identifier = identifier\n",
    "        self._model_params = sk_model_params\n",
    "\n",
    "    def train(self, X, y, not_scaled=False, **kwargs):\n",
    "        if not_scaled:\n",
    "            X = self._scaler.transform(X)\n",
    "\n",
    "        # due to the big dataset we need to check wich model was instantiated and do some model\n",
    "        # specific stuff to enable us to train the model in batches\n",
    "        if isinstance(self._model, RandomForestClassifier):\n",
    "            new_estimators = kwargs['add_estimators'] if 'add_estimators' in kwargs else 50\n",
    "            if self._model.warm_start: self._model.n_estimators += new_estimators\n",
    "            #print(f'Current estimator increased to {self._model.n_estimators}, {new_estimators} added this round.')\n",
    "        # if isinstance(self._model, SVC):\n",
    "        #     pass\n",
    "\n",
    "        self._model.fit(X, y)\n",
    "\n",
    "    def predict(self, X, not_scaled=False):\n",
    "        if not self._model: raise ValueError('Please load or train a model first.')\n",
    "        if not_scaled:\n",
    "            if not self._scaler: raise ValueError('Please load or fit a scaler first.')\n",
    "            X = self._scaler.transform(X)\n",
    "        return self._model.predict(X)\n",
    "\n",
    "    def save(self):\n",
    "        try:\n",
    "            Path.mkdir(Path('models'), exist_ok=True)\n",
    "            with open(f'models/model_{self.identifier}.jlb', 'wb') as f:\n",
    "                dump(self._model, f)\n",
    "            with open(f'models/scaler_{self.identifier}.jlb', 'wb') as f:\n",
    "                dump(self._scaler, f)\n",
    "        except:\n",
    "            raise ValueError('Unable to save model and scaler.')\n",
    "\n",
    "    def load(self, filepath):\n",
    "        try:\n",
    "            # load model and scaler\n",
    "            if os.path.isfile(filepath):\n",
    "                print(f'Loading model from {filepath}')\n",
    "                with open(filepath, 'rb') as f:\n",
    "                    self._model = load(f)\n",
    "            scaler_path = f'{os.path.split(filepath)[0]}/scaler_{os.path.split(filepath)[1].split(\".\")[0].split(\"_\")[1]}.pkl'\n",
    "            if os.path.isfile(scaler_path):\n",
    "                print(f'Loading scaler from {scaler_path}')\n",
    "                with open(filepath, 'rb') as f:\n",
    "                    self._scaler = load(f)\n",
    "\n",
    "            # extract identifier from filename\n",
    "            self._identifier = filepath.split('_')[1].split('.')[0]\n",
    "        except (FileNotFoundError) as e:\n",
    "                print(f'File {e} not found')\n",
    "                raise ValueError('Filepath is not valid. Unable to load model and scaler.')\n",
    "        except (IndexError) as e:\n",
    "                print(f'The name of the file does not implement the convention \"<model|scaler>_<some identifier>\".')\n",
    "                raise ValueError('Filepath is not valid. Unable to load model and scaler.')\n",
    "        except (Exception) as e:\n",
    "            print(f'Something went wrong. {e}')\n",
    "            raise ValueError('Unknown Error.')\n",
    "\n",
    "    def evaluate(self, X, y, scoreFx=None, not_scaled=False, prepredicted=None):\n",
    "        if not scoreFx:\n",
    "            raise ValueError('Please provide a score function.')\n",
    "        if not self._model:\n",
    "            raise ValueError('Please load or train a model first.')\n",
    "        if not_scaled and prepredicted is None:\n",
    "            if not self._scaler: raise ValueError('Please load or fit a scaler first.')\n",
    "            X = self._scaler.transform(X)\n",
    "\n",
    "        return scoreFx(y, prepredicted if prepredicted is not None else self._model.predict(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the training and the validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEW_TRAIN_DATA = '../data/engineered/train/'\n",
    "NEW_VALIDATION_DATA = '../data/engineered/val/'\n",
    "TRAIN_DATA = '../data/train_20231021'\n",
    "#TRAIN_DATA = '../data/train_20231021_20M'\n",
    "VALIDATION_DATA = '../data/validation_20231021'\n",
    "\n",
    "# figure out series id mapping\n",
    "SERIES_MAPPING_ENABLED = False\n",
    "if not 'series_id_mapping' in vars() and SERIES_MAPPING_ENABLED:\n",
    "    series_id_mapping = {'train': dict(), 'validation': dict()}\n",
    "    t = ds.dataset(NEW_TRAIN_DATA).to_table(columns=['series_id'])\n",
    "    #t = pq.ParquetDataset(TRAIN_DATA).read(columns=['series_id'])\n",
    "    for i, data in enumerate(t.to_pandas()['series_id'].unique()):\n",
    "        series_id_mapping['train'][data] = i\n",
    "    #v = pq.ParquetDataset(VALIDATION_DATA).read(columns=['series_id'])\n",
    "    v = ds.dataset(NEW_VALIDATION_DATA).to_table(columns=['series_id'])\n",
    "    for i, data in enumerate(v.to_pandas()['series_id'].unique()):\n",
    "        series_id_mapping['validation'][data] = i\n",
    "    del t, v\n",
    "    gc.collect()\n",
    "\n",
    "#train_dataset_length = pq.ParquetFile(TRAIN_DATA).metadata.num_rows\n",
    "train_dataset_length = ds.dataset(NEW_TRAIN_DATA).count_rows()\n",
    "validation_dataset_length = ds.dataset(NEW_VALIDATION_DATA).count_rows()\n",
    "\n",
    "def dataloader_full_dataset(validation=False):\n",
    "    return pd.read_parquet(VALIDATION_DATA if validation else TRAIN_DATA)\n",
    "\n",
    "def dataloader(validation=False, batch_size=5_000_000):\n",
    "    parquet_file = pq.ParquetFile(VALIDATION_DATA if validation else TRAIN_DATA)\n",
    "    for i in parquet_file.iter_batches(batch_size=batch_size):\n",
    "        yield i.to_pandas()\n",
    "\n",
    "def batched_dataloader(validation=False, batch_size=100_000):\n",
    "    dataset = ds.dataset(NEW_VALIDATION_DATA if validation else NEW_TRAIN_DATA)\n",
    "    batch = pd.DataFrame()\n",
    "    for file_batch in dataset.to_batches(batch_size=batch_size):\n",
    "        batch = pd.concat([batch, file_batch.to_pandas()])\n",
    "        if len(batch) >= batch_size:\n",
    "            yield batch.reset_index(drop=True)\n",
    "            batch = pd.DataFrame()\n",
    "    yield batch.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_from_back(path: str) -> RandomForestClassifier:\n",
    "    with open(path, 'rb') as f:\n",
    "        model = load(f)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['step', 'series_id', 'awake', 'wearable_on', 'seconds', 'year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise(ValueError('Forced stop, allows restarting the kernel and using the runn all button afterwards to get back to here.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the pipeline\n",
    "\n",
    "This pipline trains two classifiers with all possible combinations of hyperparameters, the sensor worn classifier and the sleep state classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define all the combinations of models and features\n",
    "models_and_hyperparams = {\n",
    "    RandomForestClassifier: {\n",
    "        'params': {\n",
    "            'n_estimators': [500], #, 600, 400],\n",
    "            'max_depth': [None, 20, 40, 60],\n",
    "            'min_samples_leaf': [5, 15, 25],\n",
    "            'random_state': [42],\n",
    "            'n_jobs': [10],\n",
    "            #'criterion': ['gini', 'entropy', 'log_loss'],\n",
    "            'warm_start': [True]\n",
    "        },\n",
    "        'modeltype': SkLearnModel,\n",
    "        'scaler': StandardScaler,\n",
    "        'training_type': ModelTrainingType.BATCH,\n",
    "        'batch_size': 1_500_000\n",
    "    }\n",
    "}\n",
    "\n",
    "# define scaler\n",
    "pretrained_scalers = dict() # used to store trained scalers for later use\n",
    "\n",
    "# loop over all combinations and append it to the configurations list\n",
    "# if there are no hyperparams, just instantiate the model without params\n",
    "for model_type, hyperparams in models_and_hyperparams.items():\n",
    "    if len(hyperparams['params']) > 0: # if there are hyperparams, build a dict and pass it to the model as parameters\n",
    "        fx_param_names, fx_param_values = zip(*hyperparams['params'].items())\n",
    "        for cartesian_product_values in product(*fx_param_values):\n",
    "            hyperparam_dict = dict(zip(fx_param_names, cartesian_product_values))\n",
    "\n",
    "            # create or reuse the scaler specified in the models_and_hyperparams dictionary\n",
    "            if hyperparams['scaler'] in pretrained_scalers:\n",
    "                print(f'Using pretrained scaler {hyperparams[\"scaler\"].__name__}')\n",
    "                scaler = pretrained_scalers[hyperparams['scaler']]\n",
    "            else:\n",
    "                scaler = hyperparams['scaler']()\n",
    "                print(f'Start fitting scaler {hyperparams[\"scaler\"].__name__}')\n",
    "                for batch in tqdm(batched_dataloader(batch_size=hyperparams['batch_size']) if hyperparams['training_type'] == ModelTrainingType.BATCH else dataloader_full_dataset()):\n",
    "                    X = batch.drop(columns_to_drop, axis=1)\n",
    "                    #X = batch.drop(['wearable_on', 'awake', 'wakeup', 'onset'], axis=1)\n",
    "                    #X['series_id'] = X['series_id'].map(series_id_mapping['train'])\n",
    "                    scaler.partial_fit(X)\n",
    "                    del X\n",
    "                    gc.collect()\n",
    "                # save the scaler for later use\n",
    "                pretrained_scalers[hyperparams['scaler']] = scaler\n",
    "\n",
    "            # create the model from the modeltype and the hyperparam_dict\n",
    "            m = hyperparams['modeltype'](model_type(**hyperparam_dict), \\\n",
    "                                        f'{model_type.__name__}-wrist-{\"-\".join([f\"{na}__{str(va)}\" for na, va in hyperparam_dict.items()])}'.lower(), \\\n",
    "                                        scaler, \\\n",
    "                                        hyperparam_dict)\n",
    "            m2 = hyperparams['modeltype'](model_type(**hyperparam_dict), \\\n",
    "                                        f'{model_type.__name__}-sleep-{\"-\".join([f\"{na}__{str(va)}\" for na, va in hyperparam_dict.items()])}'.lower(), \\\n",
    "                                        scaler, \\\n",
    "                                        hyperparam_dict)\n",
    "\n",
    "            # init wandb\n",
    "            # start a new wandb run to track this script\n",
    "            #cfg = m._model_params.copy()\n",
    "            #cfg['model'] = m._model.__name__\n",
    "            #wandb.init(project=\"classic_models\", config=cfg)\n",
    "\n",
    "            # train model\n",
    "            rounds = 1\n",
    "            if hyperparams['training_type'] == ModelTrainingType.BATCH:\n",
    "                print(f'Model will be traind in batches of {hyperparams[\"batch_size\"]} samples.')\n",
    "                print(f'Dataset contains {train_dataset_length} samples. Batchsize is {hyperparams[\"batch_size\"]}. That means {(rounds := train_dataset_length / hyperparams[\"batch_size\"])} batches will be used to fit the model.')\n",
    "                if model_type == RandomForestClassifier:\n",
    "                    # print(f'At the end {hyperparam_dict[\"n_estimators\"]} estimators will be fitted. That means, {hyperparam_dict[\"n_estimators\"]//rounds} estimators per batch.')\n",
    "                    m._model.n_estimators = 0 #set to 0. model will increase the number of estimators in each round\n",
    "                    m2._model.n_estimators = 0 #set to 0. model will increase the number of estimators in each round\n",
    "                    print(f'Build estimator distribution for {rounds} rounds.')\n",
    "                    est_anteil = [1 for _ in range(floor(rounds))]\n",
    "                    est_anteil.append(rounds % floor(rounds) if rounds >= 1 else 1)\n",
    "                    est_anteil = softmax(est_anteil)\n",
    "                    est_anteil *= hyperparam_dict['n_estimators']\n",
    "                    est_anteil = est_anteil.round().astype(int)\n",
    "                    print(f'Estimator distribution per round: {est_anteil}')\n",
    "\n",
    "            print(f'Start training model {m.identifier}')\n",
    "            for i, batch in enumerate(tqdm(batched_dataloader(batch_size=hyperparams['batch_size']) if hyperparams['training_type'] == ModelTrainingType.BATCH else dataloader_full_dataset(), total=floor(rounds)+1)):\n",
    "                batch.dropna(inplace=True)\n",
    "                y = batch['wearable_on']\n",
    "                X = batch.drop(columns_to_drop, axis=1)\n",
    "                #X = batch.drop(['wearable_on', 'awake'], axis=1)\n",
    "                #X['series_id'] = X['series_id'].map(series_id_mapping['train'])\n",
    "                args = {}\n",
    "                if model_type == RandomForestClassifier and hyperparams['training_type'] == ModelTrainingType.BATCH: args['add_estimators'] = est_anteil[i]\n",
    "                m.train(X, y, not_scaled=True, **args)\n",
    "                del X\n",
    "                del y\n",
    "                gc.collect()\n",
    "\n",
    "            #  evaluate model\n",
    "            print(f'Start evaluating model {m.identifier}')\n",
    "            score_value_average_precision_score, score_value_recall, score_value_precision, score_f1 = [], [], [], []\n",
    "            for validation in tqdm(batched_dataloader(batch_size=hyperparams['batch_size'], validation=True) if hyperparams['training_type'] == ModelTrainingType.BATCH else dataloader_full_dataset(validation=True), total = ceil(validation_dataset_length / hyperparams[\"batch_size\"])):\n",
    "                #validation['series_id'] = validation['series_id'].map(series_id_mapping['validation'])\n",
    "                validation.dropna(inplace=True)\n",
    "                validation_y = validation['wearable_on']\n",
    "                validation.drop(columns_to_drop, axis=1, inplace=True)\n",
    "                #validation.drop(['wearable_on', 'awake'], axis=1, inplace=True)\n",
    "                validation_y_hat = m.predict(validation, not_scaled=True)\n",
    "                # this does the same as scoreFX(y, y_hat). It is wrapped in the models evaluate function.\n",
    "                # the model itself could also do the prediction if prepredicted is None\n",
    "                # then, the first argument of the evaluate function would be the X_validation data\n",
    "                score_value_average_precision_score.append(m.evaluate(None, validation_y, scoreFx=average_precision_score, not_scaled=True, prepredicted=validation_y_hat))\n",
    "                score_value_recall.append(m.evaluate(None, validation_y, scoreFx=recall_score, not_scaled=True, prepredicted=validation_y_hat))\n",
    "                score_value_precision.append(m.evaluate(None, validation_y, scoreFx=precision_score, not_scaled=True, prepredicted=validation_y_hat))\n",
    "                score_f1.append(m.evaluate(None, validation_y, scoreFx=f1_score, not_scaled=True, prepredicted=validation_y_hat))\n",
    "                del validation, validation_y\n",
    "\n",
    "            print(sum(score_value_average_precision_score) / len(score_value_average_precision_score))\n",
    "            print(sum(score_value_recall) / len(score_value_recall))\n",
    "            print(sum(score_value_precision) / len(score_value_precision))\n",
    "            print(sum(score_f1) / len(score_f1))\n",
    "            #  save model\n",
    "            m.save()\n",
    "\n",
    "            print(f'Start training model {m2.identifier}')\n",
    "            for i, batch in enumerate(tqdm(batched_dataloader(batch_size=hyperparams['batch_size']) if hyperparams['training_type'] == ModelTrainingType.BATCH else dataloader_full_dataset(), total=floor(rounds)+1)):\n",
    "                batch.dropna(inplace=True)\n",
    "                y = batch['awake']\n",
    "                X = batch.drop(columns_to_drop, axis=1)\n",
    "                #X = batch.drop(['wearable_on', 'awake'], axis=1)\n",
    "                #X['series_id'] = X['series_id'].map(series_id_mapping['train'])\n",
    "                args = {}\n",
    "                if model_type == RandomForestClassifier and hyperparams['training_type'] == ModelTrainingType.BATCH: args['add_estimators'] = est_anteil[i]\n",
    "                worn_y_hat = m.predict(X, not_scaled=True)\n",
    "                # add worn_y_hat as new column to X\n",
    "                X['pred_worn'] = worn_y_hat\n",
    "                # filter out all rows where worn_y_hat is 0\n",
    "                X = X[X['pred_worn'] == 1]\n",
    "                y = y[y.index.isin(X.index)]\n",
    "                X = X.drop(['pred_worn'], axis=1)\n",
    "                m2.train(X, y, not_scaled=True, **args)\n",
    "                del X, y\n",
    "                gc.collect()\n",
    "\n",
    "            print(f'Start evaluating model {m2.identifier}')\n",
    "            score_value_average_precision_score_m2, score_value_recall_m2, score_value_precision_m2, f1_score_m2 = [], [], [], []\n",
    "            for validation in tqdm(batched_dataloader(batch_size=hyperparams['batch_size'], validation=True) if hyperparams['training_type'] == ModelTrainingType.BATCH else dataloader_full_dataset(validation=True), total = ceil(validation_dataset_length / hyperparams[\"batch_size\"])):\n",
    "                #validation['series_id'] = validation['series_id'].map(series_id_mapping['validation'])\n",
    "                validation.dropna(inplace=True)\n",
    "                validation_y = validation['awake']\n",
    "                validation.drop(columns_to_drop, axis=1, inplace=True)\n",
    "                #validation.drop(['wearable_on', 'awake'], axis=1, inplace=True)\n",
    "                worn_y_hat = m.predict(validation, not_scaled=True)\n",
    "                validation['pred_worn'] = worn_y_hat\n",
    "                validation = validation[validation['pred_worn'] == 1]\n",
    "                validation_y = validation_y[validation_y.index.isin(validation.index)]\n",
    "                validation = validation.drop(['pred_worn'], axis=1)\n",
    "                validation_y_hat = m2.predict(validation, not_scaled=True)\n",
    "                # this does the same as scoreFX(y, y_hat). It is wrapped in the models evaluate function.\n",
    "                # the model itself could also do the prediction if prepredicted is None\n",
    "                # then, the first argument of the evaluate function would be the X_validation data\n",
    "                score_value_average_precision_score_m2.append(m2.evaluate(None, validation_y, scoreFx=average_precision_score, not_scaled=True, prepredicted=validation_y_hat))\n",
    "                score_value_recall_m2.append(m2.evaluate(None, validation_y, scoreFx=recall_score, not_scaled=True, prepredicted=validation_y_hat))\n",
    "                score_value_precision_m2.append(m2.evaluate(None, validation_y, scoreFx=precision_score, not_scaled=True, prepredicted=validation_y_hat))\n",
    "                f1_score_m2.append(m2.evaluate(None, validation_y, scoreFx=f1_score, not_scaled=True, prepredicted=validation_y_hat))\n",
    "\n",
    "                del validation, validation_y\n",
    "\n",
    "            print(sum(score_value_average_precision_score_m2) / len(score_value_average_precision_score_m2))\n",
    "            print(sum(score_value_recall_m2) / len(score_value_recall_m2))\n",
    "            print(sum(score_value_precision_m2) / len(score_value_precision_m2))\n",
    "            print(sum(f1_score_m2) / len(f1_score_m2))\n",
    "            #  save model\n",
    "            m2.save()\n",
    "\n",
    "            #break\n",
    "\n",
    "            #wandb.finish()\n",
    "    else:\n",
    "        print(\"params in the dictionary cannot be empty. Use the standard values in the dictionary for the model\", model_type.__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sorted(list(zip(fg.columns, m2._model.feature_importances_.round(4))), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This pipline trains all hyperparameter combinations for only one model, the sleep state classifier.\n",
    "\n",
    "\n",
    "Attention! Set the variable `only_final_model` to `True` if you only want to train the final model we used during the competition.\n",
    "\n",
    "Steps to train Only the final model:\n",
    "1. Set `only_final_model` to `True`\n",
    "2. Restart the kernel and press the \"RUN ALL\" button, to run all cells\n",
    "   1. Due to the raise ValueError(), the execution stops at the right time before all model combinations are trained\n",
    "3. As soon as the kernal stopped at the raise condition, run the cell directly underneath this one\n",
    "4. Wait until the model is trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_final_model = True\n",
    "\n",
    "if only_final_model:\n",
    "    models_and_hyperparams = {\n",
    "        RandomForestClassifier: {\n",
    "            'params': {\n",
    "                'n_estimators': [500],\n",
    "                'max_depth': [None],\n",
    "                'min_samples_leaf': [15],\n",
    "                'random_state': [42],\n",
    "                'n_jobs': [10],\n",
    "                'warm_start': [True]\n",
    "            },\n",
    "            'modeltype': SkLearnModel,\n",
    "            'scaler': StandardScaler,\n",
    "            'training_type': ModelTrainingType.BATCH,\n",
    "            'batch_size': 1_500_000\n",
    "        }\n",
    "    }\n",
    "if not only_final_model:\n",
    "    # define all the combinations of models and features\n",
    "    models_and_hyperparams = {\n",
    "        RandomForestClassifier: {\n",
    "            'params': {\n",
    "                'n_estimators': [500, 400, 600, 200],\n",
    "                'max_depth': [None],\n",
    "                'min_samples_leaf': [5, 1],\n",
    "                'random_state': [42],\n",
    "                'n_jobs': [10],\n",
    "                'warm_start': [True]\n",
    "            },\n",
    "            'modeltype': SkLearnModel,\n",
    "            'scaler': StandardScaler,\n",
    "            'training_type': ModelTrainingType.BATCH,\n",
    "            'batch_size': 1_500_000\n",
    "        }\n",
    "    }\n",
    "\n",
    "# define scaler\n",
    "pretrained_scalers = dict() # used to store trained scalers for later use\n",
    "\n",
    "# loop over all combinations and append it to the configurations list\n",
    "# if there are no hyperparams, just instantiate the model without params\n",
    "for model_type, hyperparams in models_and_hyperparams.items():\n",
    "    if len(hyperparams['params']) > 0: # if there are hyperparams, build a dict and pass it to the model as parameters\n",
    "        fx_param_names, fx_param_values = zip(*hyperparams['params'].items())\n",
    "        for cartesian_product_values in product(*fx_param_values):\n",
    "            hyperparam_dict = dict(zip(fx_param_names, cartesian_product_values))\n",
    "\n",
    "            # create or reuse the scaler specified in the models_and_hyperparams dictionary\n",
    "            if hyperparams['scaler'] in pretrained_scalers:\n",
    "                print(f'Using pretrained scaler {hyperparams[\"scaler\"].__name__}')\n",
    "                scaler = pretrained_scalers[hyperparams['scaler']]\n",
    "            else:\n",
    "                scaler = hyperparams['scaler']()\n",
    "                print(f'Start fitting scaler {hyperparams[\"scaler\"].__name__}')\n",
    "                for batch in tqdm(batched_dataloader(batch_size=hyperparams['batch_size']) if hyperparams['training_type'] == ModelTrainingType.BATCH else dataloader_full_dataset()):\n",
    "                    X = batch.drop(columns_to_drop, axis=1)\n",
    "                    X.dropna(inplace=True)\n",
    "                    scaler.partial_fit(X)\n",
    "                    del X\n",
    "                    gc.collect()\n",
    "                # save the scaler for later use\n",
    "                pretrained_scalers[hyperparams['scaler']] = scaler\n",
    "\n",
    "            # create the model from the modeltype and the hyperparam_dict\n",
    "            m2 = hyperparams['modeltype'](model_type(**hyperparam_dict), \\\n",
    "                                        f'V2-{model_type.__name__}-BOTH-{\"-\".join([f\"{na}__{str(va)}\" for na, va in hyperparam_dict.items()])}'.lower(), \\\n",
    "                                        scaler, \\\n",
    "                                        hyperparam_dict)\n",
    "\n",
    "            # train model\n",
    "            rounds = 1\n",
    "            if hyperparams['training_type'] == ModelTrainingType.BATCH:\n",
    "                print(f'Model will be traind in batches of {hyperparams[\"batch_size\"]} samples.')\n",
    "                print(f'Dataset contains {train_dataset_length} samples. Batchsize is {hyperparams[\"batch_size\"]}. That means {(rounds := train_dataset_length / hyperparams[\"batch_size\"])} batches will be used to fit the model.')\n",
    "                if model_type == RandomForestClassifier:\n",
    "                    # print(f'At the end {hyperparam_dict[\"n_estimators\"]} estimators will be fitted. That means, {hyperparam_dict[\"n_estimators\"]//rounds} estimators per batch.')\n",
    "                    m2._model.n_estimators = 0 #set to 0. model will increase the number of estimators in each round\n",
    "                    # # tune hyperparameter 'n_estimators' based on the number of batches\n",
    "                    print(f'Build estimator distribution for {rounds} rounds.')\n",
    "                    est_anteil = [1 for _ in range(floor(rounds))]\n",
    "                    est_anteil.append(rounds % floor(rounds) if rounds >= 1 else 1)\n",
    "                    est_anteil = softmax(est_anteil)\n",
    "                    est_anteil *= hyperparam_dict['n_estimators']\n",
    "                    est_anteil = est_anteil.round().astype(int)\n",
    "                    print(f'Estimator distribution per round: {est_anteil}')\n",
    "\n",
    "            print(f'Start training model {m2.identifier}')\n",
    "            for i, batch in enumerate(tqdm(batched_dataloader(batch_size=hyperparams['batch_size']) if hyperparams['training_type'] == ModelTrainingType.BATCH else dataloader_full_dataset(), total=floor(rounds)+1)):\n",
    "                batch.dropna(inplace=True)\n",
    "                y = batch['awake']\n",
    "                X = batch.drop(columns_to_drop, axis=1)\n",
    "                args = {}\n",
    "                if model_type == RandomForestClassifier and hyperparams['training_type'] == ModelTrainingType.BATCH:\n",
    "                    args['add_estimators'] = est_anteil[i]\n",
    "                m2.train(X, y, not_scaled=True, **args)\n",
    "                del X, y\n",
    "                gc.collect()\n",
    "\n",
    "            print(f'Start evaluating model {m2.identifier}')\n",
    "            score_value_average_precision_score_m2, score_value_recall_m2, score_value_precision_m2, f1_score_m2 = [], [], [], []\n",
    "            for validation in tqdm(batched_dataloader(batch_size=hyperparams['batch_size'], validation=True) if hyperparams['training_type'] == ModelTrainingType.BATCH else dataloader_full_dataset(validation=True), total = ceil(validation_dataset_length / hyperparams[\"batch_size\"])):\n",
    "                validation.dropna(inplace=True)\n",
    "                validation_y = validation['awake']\n",
    "                validation.drop(columns_to_drop, axis=1, inplace=True)\n",
    "                validation_y_hat = m2.predict(validation, not_scaled=True)\n",
    "                # this does the same as scoreFX(y, y_hat). It is wrapped in the models evaluate function.\n",
    "                # the model itself could also do the prediction if prepredicted is None\n",
    "                # then, the first argument of the evaluate function would be the X_validation data\n",
    "                score_value_average_precision_score_m2.append(m2.evaluate(None, validation_y, scoreFx=average_precision_score, not_scaled=True, prepredicted=validation_y_hat))\n",
    "                score_value_recall_m2.append(m2.evaluate(None, validation_y, scoreFx=recall_score, not_scaled=True, prepredicted=validation_y_hat))\n",
    "                score_value_precision_m2.append(m2.evaluate(None, validation_y, scoreFx=precision_score, not_scaled=True, prepredicted=validation_y_hat))\n",
    "                f1_score_m2.append(m2.evaluate(None, validation_y, scoreFx=f1_score, not_scaled=True, prepredicted=validation_y_hat))\n",
    "\n",
    "                del validation, validation_y\n",
    "\n",
    "            print(sum(score_value_average_precision_score_m2) / len(score_value_average_precision_score_m2))\n",
    "            print(sum(score_value_recall_m2) / len(score_value_recall_m2))\n",
    "            print(sum(score_value_precision_m2) / len(score_value_precision_m2))\n",
    "            print(sum(f1_score_m2) / len(f1_score_m2))\n",
    "            #  save model\n",
    "            m2.save()\n",
    "\n",
    "            break\n",
    "\n",
    "            #wandb.finish()\n",
    "    else:\n",
    "        print(\"params in the dictionary cannot be empty. Use the standard values in the dictionary for the model\", model_type.__name__)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aich",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
