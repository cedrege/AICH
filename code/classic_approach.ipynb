{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to solve the kaggle competition \"Child Mind Institute - Detect Sleep States\" with a neural network\n",
    "Link to the competition: https://www.kaggle.com/competitions/child-mind-institute-detect-sleep-states\n",
    "\n",
    "\n",
    "### Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib plotly pandas numpy tqdm scikit-learn pyarrow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to switch between different models from different libraries at a glance, we implement an interface called `IPipelineRequirements`. This allows us to make the pipleine even more robust and easier to extend upon. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod #, classmethod\n",
    "import os\n",
    "\n",
    "class IPipelineRequirements(ABC):\n",
    "    @abstractmethod\n",
    "    def train(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def save(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def load(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def evaluate(self):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first model is the baseline model, which just takes the mean over all onset and wakeup times and tries to predict `onset` and `wakeup` events with the calculated time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineModel(IPipelineRequirements):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def train(self):\n",
    "        raise NotImplementedError(\"Please Implement this method\")\n",
    "\n",
    "    def predict(self):\n",
    "        raise NotImplementedError(\"Please Implement this method\")\n",
    "\n",
    "    def save(self):\n",
    "        raise NotImplementedError(\"Please Implement this method\")\n",
    "\n",
    "    def load(self):\n",
    "        raise NotImplementedError(\"Please Implement this method\")\n",
    "\n",
    "    def evaluate(self):\n",
    "        raise NotImplementedError(\"Please Implement this method\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = BaselineModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numerous studies have concentrated on applications using `RandomForest`. The primary motivation for this preference is the model's inherent transparency in decision-making processes, which are readily identifiable in such models. Subsequent to the BaselineModel, efforts have been made to abstract models from the Scikit-learn library. Fortunately, the majority of models within their API exhibit consistent implementation patterns, facilitating their integration into the processing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from pickle import dump, load\n",
    "\n",
    "class SkLearnModel(IPipelineRequirements):\n",
    "\n",
    "    def __init__(self, model_path):\n",
    "        self.load(model_path)\n",
    "\n",
    "    def __init__(self, model, identifier, scaler=StandardScaler):\n",
    "        self._model = model\n",
    "        self._scaler = scaler if not callable(scaler) else scaler() # if scaler is a class, instantiate it\n",
    "        self.identifier = identifier\n",
    "\n",
    "    def train(self, X, y, not_scaled=False, sk_fit_params=None):\n",
    "        if not not_scaled:\n",
    "            X = self._scaler().fit_transform(X)\n",
    "        if sk_fit_params:\n",
    "            self._model.fit(X, y, **sk_fit_params)\n",
    "        else:\n",
    "            self._model.fit(X, y)\n",
    "\n",
    "    def predict(self, X, not_scaled=False):\n",
    "        if not self._model: raise ValueError('Please load or train a model first.')\n",
    "        if not not_scaled:\n",
    "            if not self._scaler: raise ValueError('Please load or fit a scaler first.')\n",
    "            X = self._scaler.transform(X)\n",
    "        return self._model.predict(X)\n",
    "\n",
    "    def save(self):\n",
    "        try:\n",
    "            with open(f'model_{self.identifier}.pkl', 'wb') as f:\n",
    "                dump(self._model, f)\n",
    "            with open(f'scaler_{self.identifier}.pkl', 'wb') as f:\n",
    "                dump(self._scaler, f)\n",
    "        except:\n",
    "            raise ValueError('Unable to save model and scaler.')\n",
    "\n",
    "    def load(self, filepath):\n",
    "        try:\n",
    "            # load model and scaler\n",
    "            if os.path.isfile(filepath):\n",
    "                print(f'Loading model from {filepath}')\n",
    "                with open(filepath, 'rb') as f:\n",
    "                    self._model = load(f)\n",
    "            scaler_path = f'{os.path.split(filepath)[0]}/scaler_{os.path.split(filepath)[1].split(\".\")[0].split(\"_\")[1]}.pkl'\n",
    "            if os.path.isfile(scaler_path):\n",
    "                print(f'Loading scaler from {scaler_path}')\n",
    "                with open(filepath, 'rb') as f:\n",
    "                    self._scaler = load(f)\n",
    "\n",
    "            # extract identifier from filename\n",
    "            self._identifier = filepath.split('_')[1].split('.')[0]\n",
    "        except (FileNotFoundError) as e:\n",
    "                print(f'File {e} not found')\n",
    "                raise ValueError('Filepath is not valid. Unable to load model and scaler.')\n",
    "        except (IndexError) as e:\n",
    "                print(f'The name of the file does not implement the convention \"<model|scaler>_<some identifier>\".')\n",
    "                raise ValueError('Filepath is not valid. Unable to load model and scaler.')\n",
    "        except (Exception) as e:\n",
    "            print(f'Something went wrong. {e}')\n",
    "            raise ValueError('Unknown Error.')\n",
    "\n",
    "    def evaluate(self, X, y, scoreFx=None, not_scaled=False):\n",
    "        if not scoreFx:\n",
    "            raise ValueError('Please provide a score function.')\n",
    "        if not self._model:\n",
    "            raise ValueError('Please load or train a model first.')\n",
    "        if not not_scaled:\n",
    "            if not self._scaler: raise ValueError('Please load or fit a scaler first.')\n",
    "            X = self._scaler.transform(X)\n",
    "            \n",
    "        #TODO: Does score FX require an array of predictions or a single prediction?\n",
    "        #TODO: TEST PARAMS OF SCORE FX!!!!\n",
    "        return scoreFx(y, self.predict(X))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline base idea\n",
    "# import all models and the score function\n",
    "from itertools import product\n",
    "from tqdm import tqdm\n",
    "from score import score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR \n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# define all the combinations of models and features\n",
    "models_and_hyperparams = {\n",
    "    RandomForestClassifier: {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [2, 3, 4],\n",
    "        'random_state': [0]\n",
    "    },\n",
    "    LinearRegression: {\n",
    "\n",
    "    },\n",
    "    SVR: {\n",
    "        'kernel': ['rbf', 'poly', 'poly', 'poly', 'sigmoid'],\n",
    "        'degree': [3, 4, 5],\n",
    "        'C': [1, 10, 100, 1000]\n",
    "    },\n",
    "    PolynomialFeatures: {\n",
    "        'degree': [2, 3, 4]\n",
    "    }\n",
    "}\n",
    "\n",
    "# 1. load data\n",
    "\n",
    "\n",
    "# 2. split data\n",
    "X_train, X_test, y_train, y_test = None, None, None, None\n",
    "\n",
    "# 3. define scaler\n",
    "scaler_type = StandardScaler\n",
    "\n",
    "configurations = []\n",
    "\n",
    "# loop over all combinations and append it to the configurations list\n",
    "# if there are no hyperparams, just instantiate the model without params\n",
    "for model_type, hyperparams in models_and_hyperparams.items():\n",
    "    if len(hyperparams) > 0: # if there are hyperparams, build a dict and pass it to the model as parameters\n",
    "        fx_param_names, fx_param_values = zip(*hyperparams.items())\n",
    "        for cartesian_product_values in product(*fx_param_values):\n",
    "            m = SkLearnModel(model_type(**dict(zip(fx_param_names, cartesian_product_values))), 'XXXX', scaler_type)\n",
    "            configurations.append(m)\n",
    "    else:\n",
    "        m = SkLearnModel(model_type(), 'XXXX', scaler_type)\n",
    "        configurations.append(m)\n",
    "\n",
    "for configured_model in tqdm(configurations):\n",
    "    # 4. train models\n",
    "    configured_model.train(X_train, y_train, not_scaled=True)\n",
    "\n",
    "    #y_hat = configured_model.predict(X_test)\n",
    "\n",
    "    # 5. evaluate model\n",
    "    score_value = configured_model.evaluate(X_test, y_test, scoreFx=score, not_scaled=True)\n",
    "    print(score_value)\n",
    "\n",
    "    # 6. save model\n",
    "    configured_model.save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aich",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
