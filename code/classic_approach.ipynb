{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to solve the kaggle competition \"Child Mind Institute - Detect Sleep States\" with a neural network\n",
    "Link to the competition: https://www.kaggle.com/competitions/child-mind-institute-detect-sleep-states\n",
    "\n",
    "\n",
    "### Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in f:\\venv\\aich\\lib\\site-packages (3.8.0)\n",
      "Requirement already satisfied: plotly in f:\\venv\\aich\\lib\\site-packages (5.17.0)\n",
      "Requirement already satisfied: pandas in f:\\venv\\aich\\lib\\site-packages (2.1.1)\n",
      "Requirement already satisfied: numpy in f:\\venv\\aich\\lib\\site-packages (1.24.1)\n",
      "Requirement already satisfied: tqdm in f:\\venv\\aich\\lib\\site-packages (4.66.1)\n",
      "Requirement already satisfied: scikit-learn in f:\\venv\\aich\\lib\\site-packages (1.3.1)\n",
      "Requirement already satisfied: pyarrow in f:\\venv\\aich\\lib\\site-packages (13.0.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in f:\\venv\\aich\\lib\\site-packages (from matplotlib) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in f:\\venv\\aich\\lib\\site-packages (from matplotlib) (0.12.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in f:\\venv\\aich\\lib\\site-packages (from matplotlib) (4.43.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in f:\\venv\\aich\\lib\\site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in f:\\venv\\aich\\lib\\site-packages (from matplotlib) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in f:\\venv\\aich\\lib\\site-packages (from matplotlib) (9.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in f:\\venv\\aich\\lib\\site-packages (from matplotlib) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in f:\\venv\\aich\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in f:\\venv\\aich\\lib\\site-packages (from plotly) (8.2.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in f:\\venv\\aich\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in f:\\venv\\aich\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: colorama in f:\\venv\\aich\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: scipy>=1.5.0 in f:\\venv\\aich\\lib\\site-packages (from scikit-learn) (1.11.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in f:\\venv\\aich\\lib\\site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in f:\\venv\\aich\\lib\\site-packages (from scikit-learn) (3.2.0)\n",
      "Requirement already satisfied: six>=1.5 in f:\\venv\\aich\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib plotly pandas numpy tqdm scikit-learn pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "# wandb.login() requires you to get your API key from your account settings\n",
    "# open the weights and biases website https://wandb.ai/login and login to your account\n",
    "# then go to your account settings and copy the API key\n",
    "# paste it in the input box and hit enter\n",
    "\n",
    "#wandb.login() #TODO: uncomment this line to login to your account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # start a new wandb run to track this script\n",
    "# wandb.init(\n",
    "#     # set the wandb project where this run will be logged\n",
    "#     project=\"my-awesome-project\",\n",
    "    \n",
    "#     # track hyperparameters and run metadata\n",
    "#     config={\n",
    "#     \"learning_rate\": 0.02,\n",
    "#     \"architecture\": \"CNN\",\n",
    "#     \"dataset\": \"CIFAR-100\",\n",
    "#     \"epochs\": 10,\n",
    "#     }\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to switch between different models from different libraries at a glance, we implement an interface called `IPipelineRequirements`. This allows us to make the pipleine even more robust and easier to extend upon. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod #, classmethod\n",
    "import os\n",
    "\n",
    "class IPipelineRequirements(ABC):\n",
    "    @abstractmethod\n",
    "    def train(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def save(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def load(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def evaluate(self):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first model is the baseline model, which just takes the mean over all onset and wakeup times and tries to predict `onset` and `wakeup` events with the calculated time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineModel(IPipelineRequirements):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def train(self):\n",
    "        raise NotImplementedError(\"Please Implement this method\")\n",
    "\n",
    "    def predict(self):\n",
    "        raise NotImplementedError(\"Please Implement this method\")\n",
    "\n",
    "    def save(self):\n",
    "        raise NotImplementedError(\"Please Implement this method\")\n",
    "\n",
    "    def load(self):\n",
    "        raise NotImplementedError(\"Please Implement this method\")\n",
    "\n",
    "    def evaluate(self):\n",
    "        raise NotImplementedError(\"Please Implement this method\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = BaselineModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numerous studies have concentrated on applications using `RandomForest`. The primary motivation for this preference is the model's inherent transparency in decision-making processes, which are readily identifiable in such models. Subsequent to the BaselineModel, efforts have been made to abstract models from the Scikit-learn library. Fortunately, the majority of models within their API exhibit consistent implementation patterns, facilitating their integration into the processing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from pickle import dump, load\n",
    "\n",
    "class SkLearnModel(IPipelineRequirements):\n",
    "\n",
    "    def __init__(self, model_path):\n",
    "        self.load(model_path)\n",
    "\n",
    "    def __init__(self, model, identifier, scaler=StandardScaler, sk_model_params=None):\n",
    "        self._model = model\n",
    "        self._scaler = scaler if not callable(scaler) else scaler() # if scaler is a classpointer, instantiate it\n",
    "        self.identifier = identifier\n",
    "        self._model_params = sk_model_params\n",
    "\n",
    "    def set_pretrained_scaler(self, scaler):\n",
    "        self._scaler = scaler\n",
    "\n",
    "    def train(self, X, y, not_scaled=False, sk_fit_params=None):\n",
    "        if not_scaled:\n",
    "            X = self._scaler.transform(X)\n",
    "\n",
    "        # due to the big dataset we need to check wich model was instantiated and do some model\n",
    "        # specific stuff to enable us to train the model in batches\n",
    "        if isinstance(self._model, RandomForestClassifier):\n",
    "            added_estimators = 75\n",
    "            if self._model.warm_start: self._model.n_estimators += added_estimators\n",
    "            print(f'Current estimator increased to {self._model.n_estimators}, {added_estimators} added this round.')\n",
    "        if isinstance(self._model, SVC):\n",
    "            pass\n",
    "\n",
    "        if sk_fit_params:\n",
    "            self._model.fit(X, y, **sk_fit_params)\n",
    "        else:\n",
    "            self._model.fit(X, y)\n",
    "\n",
    "    def predict(self, X, not_scaled=False):\n",
    "        if not self._model: raise ValueError('Please load or train a model first.')\n",
    "        if not_scaled:\n",
    "            if not self._scaler: raise ValueError('Please load or fit a scaler first.')\n",
    "            X = self._scaler.transform(X)\n",
    "        return self._model.predict(X)\n",
    "\n",
    "    def save(self):\n",
    "        try:\n",
    "            with open(f'model_{self.identifier}.pkl', 'wb') as f:\n",
    "                dump(self._model, f)\n",
    "            with open(f'scaler_{self.identifier}.pkl', 'wb') as f:\n",
    "                dump(self._scaler, f)\n",
    "        except:\n",
    "            raise ValueError('Unable to save model and scaler.')\n",
    "\n",
    "    def load(self, filepath):\n",
    "        try:\n",
    "            # load model and scaler\n",
    "            if os.path.isfile(filepath):\n",
    "                print(f'Loading model from {filepath}')\n",
    "                with open(filepath, 'rb') as f:\n",
    "                    self._model = load(f)\n",
    "            scaler_path = f'{os.path.split(filepath)[0]}/scaler_{os.path.split(filepath)[1].split(\".\")[0].split(\"_\")[1]}.pkl'\n",
    "            if os.path.isfile(scaler_path):\n",
    "                print(f'Loading scaler from {scaler_path}')\n",
    "                with open(filepath, 'rb') as f:\n",
    "                    self._scaler = load(f)\n",
    "\n",
    "            # extract identifier from filename\n",
    "            self._identifier = filepath.split('_')[1].split('.')[0]\n",
    "        except (FileNotFoundError) as e:\n",
    "                print(f'File {e} not found')\n",
    "                raise ValueError('Filepath is not valid. Unable to load model and scaler.')\n",
    "        except (IndexError) as e:\n",
    "                print(f'The name of the file does not implement the convention \"<model|scaler>_<some identifier>\".')\n",
    "                raise ValueError('Filepath is not valid. Unable to load model and scaler.')\n",
    "        except (Exception) as e:\n",
    "            print(f'Something went wrong. {e}')\n",
    "            raise ValueError('Unknown Error.')\n",
    "\n",
    "    def evaluate(self, X, y, scoreFx=None, not_scaled=False):\n",
    "        if not scoreFx:\n",
    "            raise ValueError('Please provide a score function.')\n",
    "        if not self._model:\n",
    "            raise ValueError('Please load or train a model first.')\n",
    "        if not_scaled:\n",
    "            if not self._scaler: raise ValueError('Please load or fit a scaler first.')\n",
    "            X = self._scaler.transform(X)\n",
    "            \n",
    "        #TODO: Does score FX require an array of predictions or a single prediction?\n",
    "        #TODO: TEST PARAMS OF SCORE FX!!!!\n",
    "        return scoreFx(y, self._model.predict(X))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heuristics method to get the right event under the constraints given by Child Mind Institue\n",
    "\n",
    "* A single sleep period must be at least 30 minutes in length\n",
    "* A single sleep period can be interrupted by bouts of activity that do not exceed 30 consecutive minutes\n",
    "* No sleep windows can be detected unless the watch is deemed to be worn for the duration (elaborated upon, below)\n",
    "* The longest sleep window during the night is the only one which is recorded\n",
    "* If no valid sleep window is identifiable, neither an onset nor a wakeup event is recorded for that night.\n",
    "* Sleep events do not need to straddle the day-line, and therefore there is no hard rule defining how many may occur within a given period. However, no more than one window should be assigned per night. For example, it is valid for an individual to have a sleep window from 01h00–06h00 and 19h00–23h30 in the same calendar day, though assigned to consecutive nights\n",
    "* There are roughly as many nights recorded for a series as there are 24-hour periods in that series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heuristic_filter(data):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the training and the validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "# import pandas as pd\n",
    "# import pyarrow.parquet as pq\n",
    "# import gc\n",
    "\n",
    "# parquet_file = pq.ParquetFile('../data/train_20231021')\n",
    "# for i in parquet_file.iter_batches(batch_size=5_000_000):\n",
    "#     train = i.to_pandas()\n",
    "#     break\n",
    "\n",
    "\n",
    "# train = pd.read_parquet('../data/train_20231021')\n",
    "# validation = pd.read_parquet('../data/validation_20231021')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# series_id_mapping = dict()\n",
    "# for i, data in enumerate(train.series_id.unique()):\n",
    "#     series_id_mapping[data] = i\n",
    "# train['series_id'] = train['series_id'].map(series_id_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # fit the scaler\n",
    "# scr = StandardScaler()\n",
    "# for serie in tqdm(train.series_id.unique()):\n",
    "#     X = train[train.series_id == serie].drop(['wearable_on', 'awake', 'wakeup', 'onset'], axis=1)\n",
    "#     #y = train[train.series_id == serie]['wearable_on']\n",
    "#     scr.partial_fit(X)\n",
    "#     del X\n",
    "#     #del y\n",
    "#     gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # train the model\n",
    "# testmd = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=0,warm_start=True)\n",
    "# for serie in tqdm(train.series_id.unique()):\n",
    "#     X = train[train.series_id == serie].drop(['wearable_on', 'awake', 'wakeup', 'onset'], axis=1)\n",
    "#     y = train[train.series_id == serie]['wearable_on']\n",
    "#     testmd.fit(scr.transform(X), y)\n",
    "#     testmd.n_estimators += 10\n",
    "#     del X\n",
    "#     del y\n",
    "#     gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(f'TEST.pkl', 'wb') as f:\n",
    "#    dump(testmd, f)\n",
    "#with open(f'TEST_SCALER.pkl', 'wb') as f:\n",
    "#    dump(scr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation = pd.read_parquet('../data/validation_20231021')\n",
    "# validation = validation[[\"step\", \"anglez\", \"enmo\", \"hour\", \"minute\", \"seconds\", \"day\", \"month\", \"year\", \"seconds_from_midnight\", \"awake\", \"wearable_on\", \"series_id\", \"onset\", \"wakeup\"]]\n",
    "# series_id_mapping_va = dict()\n",
    "# for i, data in enumerate(validation.series_id.unique()):\n",
    "#     series_id_mapping_va[data] = i\n",
    "# validation['series_id'] = validation['series_id'].map(series_id_mapping_va)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res_f1 = []\n",
    "# res_recall = []\n",
    "# res_precision = []\n",
    "\n",
    "# for serie in tqdm(validation.series_id.unique()):\n",
    "#     X = validation[validation.series_id == serie].drop(['wearable_on', 'awake', 'wakeup', 'onset'], axis=1)\n",
    "#     y = validation[validation.series_id == serie]['wearable_on']\n",
    "#     y_pred = testmd.predict(scr.transform(X))\n",
    "#     res_f1.append(f1_score(y, y_pred))\n",
    "#     res_recall.append(recall_score(y, y_pred))\n",
    "#     res_precision.append(precision_score(y, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sum(res_f1)/len(res_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sum(res_precision)/len(res_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sum(res_recall)/len(res_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.DataFrame(y_pred).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "test",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mf:\\git\\AICH\\code\\classic_approach.ipynb Cell 27\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/f%3A/git/AICH/code/classic_approach.ipynb#X35sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mraise\u001b[39;00m(\u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m'\u001b[39m))\n",
      "\u001b[1;31mValueError\u001b[0m: test"
     ]
    }
   ],
   "source": [
    "raise(ValueError('test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import gc\n",
    "\n",
    "TRAIN_DATA = '../data/train_20231021'\n",
    "TRAIN_DATA = '../data/train_20231021_20M'\n",
    "VALIDATION_DATA = '../data/validation_20231021'\n",
    "\n",
    "# figure out series id mapping\n",
    "if not 'series_id_mapping' in vars():\n",
    "    series_id_mapping = {'train': dict(), 'validation': dict()}\n",
    "    t = pq.ParquetDataset(TRAIN_DATA).read(columns=['series_id'])\n",
    "    for i, data in enumerate(t.to_pandas()['series_id'].unique()):\n",
    "        series_id_mapping['train'][data] = i\n",
    "    v = pq.ParquetDataset(VALIDATION_DATA).read(columns=['series_id'])\n",
    "    for i, data in enumerate(v.to_pandas()['series_id'].unique()):\n",
    "        series_id_mapping['validation'][data] = i\n",
    "    del t, v\n",
    "    gc.collect()\n",
    "\n",
    "def dataloader_full_dataset(validation=False):\n",
    "    return pd.read_parquet(VALIDATION_DATA if validation else TRAIN_DATA)\n",
    "\n",
    "def dataloader(validation=False, batch_size=5_000_000):\n",
    "    parquet_file = pq.ParquetFile(VALIDATION_DATA if validation else TRAIN_DATA)\n",
    "    for i in parquet_file.iter_batches(batch_size=batch_size):\n",
    "        yield i.to_pandas()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training model randomforestclassifier-n_estimators__0-max_depth__10-random_state__0-warm_start__true\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current estimator increased to 75, 75 added this round.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [06:17, 377.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current estimator increased to 150, 75 added this round.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [13:30, 410.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current estimator increased to 225, 75 added this round.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [20:16, 408.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current estimator increased to 300, 75 added this round.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [26:55, 403.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start evaluating model randomforestclassifier-n_estimators__0-max_depth__10-random_state__0-warm_start__true\n",
      "0.6638572042117677\n",
      "0.9785789916554606\n",
      "0.6640060035666919\n"
     ]
    }
   ],
   "source": [
    "# pipeline base idea\n",
    "# import all models and the score function\n",
    "from itertools import product\n",
    "from tqdm import tqdm\n",
    "from score import score\n",
    "#from sklearn.linear_model import LinearRegression\n",
    "#from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score, average_precision_score\n",
    "\n",
    "# define all the combinations of models and features\n",
    "models_and_hyperparams = {\n",
    "    RandomForestClassifier: {\n",
    "        'params': {\n",
    "            'n_estimators': [0, 100, 200],\n",
    "            #'n_estimators': [200, 300, 400],\n",
    "            'max_depth': [10, 10, 20, None],\n",
    "            'random_state': [0],\n",
    "            #'criterion': ['gini', 'entropy', 'log_loss],\n",
    "            'warm_start': [True]\n",
    "        },\n",
    "        'modeltype': SkLearnModel,\n",
    "        'scaler': StandardScaler\n",
    "    },\n",
    "    # SVC: {\n",
    "    #     'params': {\n",
    "    #         'kernel': ['rbf', 'poly', 'poly', 'poly', 'sigmoid'],\n",
    "    #         'degree': [3, 4, 5],\n",
    "    #         'C': [1, 10, 100, 1000],\n",
    "    #         'gamma': ['scale', 'auto']\n",
    "    #     },\n",
    "    #     'modeltype': SkLearnModel,\n",
    "    #     'scaler': StandardScaler\n",
    "    # },\n",
    "    # KNeighborsClassifier: {\n",
    "    #     'params': {\n",
    "    #         'n_neighbors': [5, 10, 15, 20],\n",
    "    #         'weights': ['uniform', 'distance'],\n",
    "    #         'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "    #         'leaf_size': [10, 20, 30, 40, 50],\n",
    "    #         'p': [1, 2]\n",
    "    #     },\n",
    "    #     'modeltype': SkLearnModel,\n",
    "    #     'scaler': StandardScaler\n",
    "    # }\n",
    "}\n",
    "\n",
    "# split data\n",
    "X_train, X_test, y_train, y_test = None, None, None, None\n",
    "\n",
    "# define scaler\n",
    "scaler_type = StandardScaler\n",
    "pretrained_scalers = dict() # used to store trained scalers for later use\n",
    "\n",
    "# loop over all combinations and append it to the configurations list\n",
    "# if there are no hyperparams, just instantiate the model without params\n",
    "for model_type, hyperparams in models_and_hyperparams.items():\n",
    "    if len(hyperparams['params']) > 0: # if there are hyperparams, build a dict and pass it to the model as parameters\n",
    "        fx_param_names, fx_param_values = zip(*hyperparams['params'].items())\n",
    "        for cartesian_product_values in product(*fx_param_values):\n",
    "            hyperparam_dict = dict(zip(fx_param_names, cartesian_product_values))\n",
    "\n",
    "            # create or reuse the scaler specified in the models_and_hyperparams dictionary\n",
    "            # if hyperparams['scaler'] in pretrained_scalers:\n",
    "            #     print(f'Using pretrained scaler {hyperparams[\"scaler\"].__name__}')\n",
    "            #     scaler = pretrained_scalers[hyperparams['scaler']]\n",
    "            # else:\n",
    "            #     scaler = hyperparams['scaler']()\n",
    "            #     print(f'Start fitting scaler {hyperparams[\"scaler\"].__name__}')\n",
    "            #     for batch in tqdm(dataloader(batch_size=15_000_000)):\n",
    "            #         X = batch.drop(['wearable_on', 'awake', 'wakeup', 'onset'], axis=1)\n",
    "            #         X['series_id'] = X['series_id'].map(series_id_mapping['train'])\n",
    "            #         scaler.partial_fit(X)\n",
    "            #         del X\n",
    "            #         gc.collect()\n",
    "            #     # save the scaler for later use\n",
    "            #     pretrained_scalers[hyperparams['scaler']] = scaler\n",
    "\n",
    "            # create the model from the modeltype and the hyperparam_dict\n",
    "            m = hyperparams['modeltype'](model_type(**hyperparam_dict), \\\n",
    "                                        f'{model_type.__name__}-{\"-\".join([f\"{na}__{str(va)}\" for na, va in hyperparam_dict.items()])}'.lower(), \\\n",
    "                                        #pretrained_scalers[hyperparams['scaler']], \\\n",
    "                                        StandardScaler(), \\\n",
    "                                        hyperparam_dict)\n",
    "            \n",
    "\n",
    "#TODO: uncemment, standardscaler , not_scaled=True\n",
    "\n",
    "\n",
    "            \n",
    "            # init wandb\n",
    "            # start a new wandb run to track this script\n",
    "            #cfg = m._model_params.copy()\n",
    "            #cfg['model'] = m._model.__name__\n",
    "            #wandb.init(project=\"classic_models\", config=cfg)\n",
    "\n",
    "            # train model\n",
    "            print(f'Start training model {m.identifier}')\n",
    "            for batch in tqdm(dataloader(batch_size=5_000_000)):\n",
    "                X = batch.drop(['wearable_on', 'awake', 'wakeup', 'onset'], axis=1)\n",
    "                X['series_id'] = X['series_id'].map(series_id_mapping['train'])\n",
    "                y = batch['wearable_on']\n",
    "                m.train(X, y, not_scaled=False)\n",
    "                del X\n",
    "                del y\n",
    "                gc.collect()\n",
    "\n",
    "            #y_hat = configured_model.predict(X_test)\n",
    "\n",
    "            #  do the heuristic part...\n",
    "\n",
    "            #  evaluate model\n",
    "            print(f'Start evaluating model {m.identifier}')\n",
    "            validation = dataloader_full_dataset(validation=True)\n",
    "            validation['series_id'] = validation['series_id'].map(series_id_mapping['validation'])\n",
    "            validation_y = validation['wearable_on']\n",
    "            validation.drop(['wearable_on', 'awake', 'wakeup', 'onset'], axis=1, inplace=True)\n",
    "            score_value_average_precision_score = m.evaluate(validation, validation_y, scoreFx=average_precision_score, not_scaled=False)\n",
    "            score_value_recall = m.evaluate(validation, validation_y, scoreFx=recall_score, not_scaled=False)\n",
    "            score_value_precision = m.evaluate(validation, validation_y, scoreFx=precision_score, not_scaled=False)\n",
    "            print(score_value_average_precision_score)\n",
    "            print(score_value_recall)\n",
    "            print(score_value_precision)\n",
    "            del validation, validation_y\n",
    "\n",
    "            #  save model\n",
    "            #m.save()\n",
    "\n",
    "\n",
    "            # delete model to free up memory\n",
    "            #del m\n",
    "\n",
    "            break\n",
    "\n",
    "            #wandb.finish()\n",
    "    else:\n",
    "        print(\"params in the dictionary cannot be empty. Use the standard values in the dictionary for the model\", model_type.__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise ValueError('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.40547154, -2.26902544, -0.2318546 , -0.07499694, -1.70340755,\n",
       "       -1.59325501, -1.12511605,  0.42450226, -0.0196756 , -0.14689035,\n",
       "       -1.75911204])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#m._scaler.transform(X)[0]\n",
    "#TODO: LOAD alL DATA AND TRY TO SCALE IT? look at the first row if the data are scaeld equally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'validation' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mf:\\git\\AICH\\code\\classic_approach.ipynb Cell 32\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/f%3A/git/AICH/code/classic_approach.ipynb#X43sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdel\u001b[39;00m validation\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/git/AICH/code/classic_approach.ipynb#X43sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m gc\u001b[39m.\u001b[39mcollect()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'validation' is not defined"
     ]
    }
   ],
   "source": [
    "del validation\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "StandardScaler in pretrained_scalers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_value_f1 = m.evaluate(validation, validation_y, scoreFx=f1_score, not_scaled=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8032802541617328"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_value_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_value_recall = m.evaluate(validation, validation_y, scoreFx=recall_score, not_scaled=True)\n",
    "score_value_precision = m.evaluate(validation, validation_y, scoreFx=precision_score, not_scaled=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8032802541617328\n",
      "0.9884572274792981\n",
      "0.676538032360052\n"
     ]
    }
   ],
   "source": [
    "print(score_value_f1)\n",
    "print(score_value_recall)\n",
    "print(score_value_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA = '../data/train_20231021'\n",
    "VALIDATION_DATA = '../data/validation_20231021'\n",
    "def dataloader_full_dataset(validation=False):\n",
    "    return pd.read_parquet(VALIDATION_DATA if validation else TRAIN_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pipeline base idea\n",
    "# import all models and the score function\n",
    "from itertools import product\n",
    "from tqdm import tqdm\n",
    "from score import score\n",
    "#from sklearn.linear_model import LinearRegression\n",
    "#from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score\n",
    "import pandas as pd\n",
    "import gc\n",
    "\n",
    "t1 = RandomForestClassifier(n_estimators=50, max_depth=2, random_state=0)\n",
    "\n",
    "dd = dataloader_full_dataset(validation=False)\n",
    "dd.drop(['awake', 'wakeup', 'onset'], axis=1, inplace=True)\n",
    "df = dd['wearable_on']\n",
    "dd['series_id'] = dd['series_id'].map(series_id_mapping['train'])\n",
    "dd.drop(['wearable_on'], axis=1, inplace=True)\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(max_depth=2, n_estimators=50, random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(max_depth=2, n_estimators=50, random_state=0)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(max_depth=2, n_estimators=50, random_state=0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1.fit(dd, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import dump, load\n",
    "with open(f'model_all_data_no_scaling.pkl', 'wb') as f:\n",
    "    dump(t1, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation = dataloader_full_dataset(validation=True)\n",
    "validation['series_id'] = validation['series_id'].map(series_id_mapping['validation'])\n",
    "validation_y = validation['wearable_on']\n",
    "validation.drop(['wearable_on', 'awake', 'wakeup', 'onset'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8023151368599453\n",
      "0.6705377647348899\n",
      "0.9985563264533389\n"
     ]
    }
   ],
   "source": [
    "f1 = t1.predict(validation)\n",
    "score_value_f1 = f1_score(f1, validation_y)\n",
    "score_value_recall = recall_score(f1, validation_y)\n",
    "score_value_precision = precision_score(f1, validation_y)\n",
    "print(score_value_f1)\n",
    "print(score_value_recall)\n",
    "print(score_value_precision)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aich",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
